{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'training.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-514ca3cfbfda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreviews\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m \u001b[0mrev_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloadData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[0mrev_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloadTrainData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'testing.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-514ca3cfbfda>\u001b[0m in \u001b[0;36mloadData\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m    \u001b[1;31m# count2 = 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m        \u001b[1;31m# count2 = count2 + 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'training.txt'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from sklearn import linear_model\n",
    "\n",
    "fileWriter = open('out.txt','w')\n",
    "\n",
    "mystopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n",
    "def loadData(fname):\n",
    "    reviews=[]\n",
    "    labels=[]\n",
    "   # count2 = 0\n",
    "    f=open(fname)\n",
    "    for line in f:\n",
    "       # count2 = count2 + 1\n",
    "        review,rating=line.strip().split('\\t')\n",
    "        review = re.sub('not ', 'not', review)\n",
    "        review = re.sub('Not ', 'Not', review)\n",
    "        review = re.sub('<br>', ' ',review)\n",
    "        review = re.sub(' +', ' ',review)\n",
    "       # review = re.sub('[^a-z\\d]', ' ',review)\n",
    "        terms = review.split()\n",
    "        reviews.append(review.lower())    \n",
    "        labels.append(int(rating))\n",
    "    threegrams = ngrams(terms,3)\n",
    "    for tg in threegrams:\n",
    "        if tg[0] in mystopwords or tg[1] in mystopwords or tg[2] in mystopwords:\n",
    "            continue\n",
    "  #  print count2  \n",
    "    f.close()\n",
    "    return reviews,labels\n",
    "\n",
    "def loadTrainData(fname):\n",
    "    reviews=[]\n",
    "    f=open(fname)\n",
    "    for line in f:\n",
    "        review=line.strip()\n",
    "        review = re.sub('not ', 'not', review)\n",
    "        review = re.sub('Not ', 'Not', review)\n",
    "        review = re.sub('<br>', ' ',review)\n",
    "        review = re.sub(' +', ' ',review)\n",
    "       # review = re.sub('[^a-z\\d]', ' ',review)\n",
    "        terms = review.split()\n",
    "        reviews.append(review.lower())\n",
    "    threegrams = ngrams(terms,3)\n",
    "    for tg in threegrams:\n",
    "        if tg[0] in mystopwords or tg[1] in mystopwords or tg[2] in mystopwords:\n",
    "            continue\n",
    "    f.close()\n",
    "    return reviews\n",
    "\n",
    "rev_train,labels_train=loadData('train.tsv')\n",
    "rev_test=loadTrainData('test.tsv')\n",
    "\n",
    "\n",
    "MNB_pipeline = Pipeline([('vect', CountVectorizer(ngram_range = (1, 2))), \n",
    "                         ('clf', MultinomialNB(alpha = 1.0, fit_prior = True)),\n",
    "                        ])\n",
    "\n",
    "KNN_pipeline = Pipeline([('vect', CountVectorizer()), \n",
    "                         ('clf', KNeighborsClassifier(n_neighbors = 20)),\n",
    "                        ])\n",
    "                        \n",
    "SGD_pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                        ('clf', linear_model.SGDClassifier(loss='log')),\n",
    "                        ])\n",
    "                        \n",
    "LR_pipeline = Pipeline([('vect', CountVectorizer()), \n",
    "                        ('tfidf', TfidfTransformer(norm = 'l2', use_idf = True, smooth_idf = True, sublinear_tf = True)),\n",
    "                        ('clf', LogisticRegression(warm_start = True, random_state = 1)),\n",
    "                       ]) \n",
    "                     \n",
    "\n",
    "eclf = VotingClassifier(estimators=[('MNB', MNB_pipeline), ('SGD',SGD_pipeline), ('LR', LR_pipeline)], voting = 'soft', weights = [3,2,3])\n",
    "#('KNN', KNN_pipeline), \n",
    "\n",
    "eclf.fit(rev_train,labels_train)\n",
    "\n",
    "#use soft voting to predict (majority voting)\n",
    "pred=eclf.predict(rev_test)\n",
    "\n",
    "for x in pred:\n",
    "    fileWriter.write(str(x)+'\\n')\n",
    "fileWriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
